{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "21sa81U8UrXs"
      },
      "source": [
        "## 1 What is PyTorch?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "peFQh80wUrX4"
      },
      "source": [
        "PyTorch is an open source deep learning framework for Python that was developed mainly by Facebook’s AI research group."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Sl5sD9RWUrX6"
      },
      "source": [
        "## 2 Why Should We Learn PyTorch?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0n_p8Sw1UrX7"
      },
      "source": [
        "<ul>\n",
        "  <li>PyTorch provides a rich library of deep learning models.\n",
        "  <li>Numpy cannot utilize GPUs to accelerate its numerical computations. For modern deep neural networks, GPUs often provide speedups of 50x or greater, so unfortunately numpy won’t be enough for modern deep learning.</li>\n",
        "    \n",
        "  <li>PyTorch also provides tools to calculate automatic differentiation. Hence we do not need to mannually calculate the gradients for updating parameter on our neural network model.</li>\n",
        "</ul>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GvEDjPqqUrX9"
      },
      "source": [
        "## 3 PyTorch versions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "doJa3NAkUrX-",
        "outputId": "e432723f-1580-42d9-f0a4-4882554387d5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2.0.1+cu118\n"
          ]
        }
      ],
      "source": [
        "import torch \n",
        "import numpy as np\n",
        "print(torch.__version__)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aVTwKiYPUrYD"
      },
      "source": [
        "## 4 PyTorch Tensor"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VBmUQpGFUrYE"
      },
      "source": [
        "Tensors are a specialized data structure that are very similar to arrays and matrices. In PyTorch, we use tensors to encode the inputs and outputs of a model, as well as the model’s parameters.\n",
        "\n",
        "Tensors are similar to NumPy’s `ndarrays`, except that tensors can run on GPUs or other specialized hardware to accelerate computing. Tensors are also optimized for automatic differentiation (we’ll see more about that later)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MUgbogL_UrYG"
      },
      "source": [
        "### 4.1 Tensor Initialization\n",
        "Tensors can be initialized in various ways. Let us take a look at the following examples:\n",
        "#### 4.1.1 Directly from data\n",
        "Tensors can be created directly from data. The data type is automatically inferred."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "y_bZIoFGUrYH",
        "outputId": "5c5d4449-5bb5-4882-a9ff-cb032d37a8c9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.int64\n"
          ]
        }
      ],
      "source": [
        "data = [[1, 2],[3, 4]]\n",
        "# data = (1, 2)\n",
        "# print(type(data))\n",
        "x_data = torch.tensor(data)\n",
        "print(x_data.dtype)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "x_data"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "V5tPXis7LKQN",
        "outputId": "f4c2d01f-a19a-4112-f050-632df4d7440f"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[1, 2],\n",
              "        [3, 4]])"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LO8PmLh_UrYJ"
      },
      "source": [
        "#### 4.1.2 From another tensor:\n",
        "\n",
        "The new tensor retains the properties (shape, datatype) of the argument tensor, unless explicitly overridden."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "H-DH2ZtEUrYK",
        "outputId": "a7eaa073-d874-45ab-a074-33642c9bb485"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "x_ones: tensor([[1, 1],\n",
            "        [1, 1]])\n",
            "x_rand: tensor([[0.6753, 0.9933],\n",
            "        [0.1705, 0.7763]])\n"
          ]
        }
      ],
      "source": [
        "x_ones = torch.ones_like(x_data) # retains the properties of x_data\n",
        "print(\"x_ones:\",x_ones)\n",
        "\n",
        "x_rand = torch.rand_like(x_data, dtype=torch.float32) # overrides the datatype of x_data\n",
        "print(\"x_rand:\",x_rand)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ggBk6NuVUrYL"
      },
      "source": [
        "### 4.3 Data Types in PyTorch \n",
        "\n",
        "8-bit integer (unsigned) `torch.uint8` \\\n",
        "8-bit integer (signed) `torch.int8` \\\n",
        "16-bit integer (signed) `torch.int16` or `torch.short` \\\n",
        "32-bit floating point `torch.float32` or `torch.float` \\\n",
        "64-bit floating point    `torch.float64` or `torch.double` \n",
        "\n",
        "[Documentation](https://pytorch.org/docs/stable/tensors.html)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JvpAtY-fUrYM"
      },
      "source": [
        "### 4.3 PyTorch Numpy interoperability\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "#### 4.3.1 NumPy array From a Torch Tensor"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EPMu5HA5UrYN",
        "outputId": "48b62f13-257b-42fc-e616-edf6a459504e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "n: [1. 1. 1. 1. 1.]\n",
            "type(n): <class 'numpy.ndarray'>\n"
          ]
        }
      ],
      "source": [
        "t = torch.ones(5)\n",
        "# print(\"t:\",t)\n",
        "# print(\"type(t):\",type(t))\n",
        "\n",
        "n = t.numpy()\n",
        "print(\"n:\",n)\n",
        "print(\"type(n):\",type(n))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zhgJ9GdFUrYO"
      },
      "source": [
        "#### 4.3.2 Torch Tensor From a NumPy array"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "BlXx9ZwHUrYP"
      },
      "outputs": [],
      "source": [
        "n = np.ones(5)\n",
        "t = torch.from_numpy(n)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uncr_nosUrYP"
      },
      "source": [
        "### 4.4 With random or constant values:\n",
        "\n",
        "``shape`` is a tuple of tensor dimensions. In the functions below, it determines the dimensionality of the output tensor.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7Bjdc2DRUrYQ",
        "outputId": "23b475b4-a9f2-4499-b2b6-73c384f7d396"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Random Tensor: tensor([[0.0668, 0.2503, 0.4027],\n",
            "        [0.6432, 0.2573, 0.8080]])\n",
            "Ones Tensor: tensor([[1., 1., 1.],\n",
            "        [1., 1., 1.]])\n",
            "Zeros Tensor: tensor([[0., 0., 0.],\n",
            "        [0., 0., 0.]])\n"
          ]
        }
      ],
      "source": [
        "shape = (2,3,)#can be list also\n",
        "rand_tensor = torch.rand(shape)\n",
        "ones_tensor = torch.ones(shape)\n",
        "zeros_tensor = torch.zeros(shape)\n",
        "\n",
        "print(\"Random Tensor:\", rand_tensor)\n",
        "print(\"Ones Tensor:\", ones_tensor)\n",
        "print(\"Zeros Tensor:\", zeros_tensor)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_aorcwv_UrYS"
      },
      "source": [
        "### 4.5 Attributes of a Tensor\n",
        "\n",
        "Tensor attributes describe their shape, datatype, and the device on which they are stored.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7EQntjc_UrYU",
        "outputId": "3be160b9-4f1c-45cd-c1b3-52f392c0adc1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Shape of tensor: torch.Size([3, 4])\n",
            "Datatype of tensor: torch.float32\n",
            "Device tensor is stored on: cpu\n"
          ]
        }
      ],
      "source": [
        "tensor = torch.rand(3,4)\n",
        "\n",
        "print(\"Shape of tensor:\",tensor.shape)\n",
        "print(\"Datatype of tensor:\",tensor.dtype)\n",
        "print(\"Device tensor is stored on:\",tensor.device)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NuitW_ziUrYV"
      },
      "source": [
        "### 4.6 Operations on Tensors\n",
        "\n",
        "Many tensor operations, including arithmetic, linear algebra, matrix manipulation (transposing,\n",
        "indexing) and more are\n",
        "comprehensively described `here <https://pytorch.org/docs/stable/torch.html>`__.\n",
        "\n",
        "Each of these operations can be run on the GPU (at typically higher speeds than on a\n",
        "CPU). \n",
        "\n",
        "> **NB** If you're running this notebook on Colab, to enable the GPU support go to `'Edit'->'Notebook settings'` and set `'Hardware accelerator'` to `'GPU'`.\n",
        "\n",
        "By default, tensors are created on the CPU. We need to explicitly move tensors to the GPU using\n",
        "``.to`` method (after checking for GPU availability).\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "device"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yxs3CTVuaMn-",
        "outputId": "f22d7c1c-f2a7-4c3e-e9c0-142420fc36c1"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "device(type='cuda', index=0)"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1PRQpzhVUrYV",
        "outputId": "d14158d7-c573-41fb-877e-e7bdb43e2287"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Device tensor is stored on: cuda:0\n"
          ]
        }
      ],
      "source": [
        "# We move our tensor to the GPU if available\n",
        "\n",
        "tensor = tensor.to(device)\n",
        "print(\"Device tensor is stored on:\",tensor.device)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yM4nnd7FUrYW"
      },
      "source": [
        "### 4.7 Standard numpy-like indexing and slicing:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gesL_jFOUrYX",
        "outputId": "bf962ae0-a338-49e1-b37b-a83ba90da0bc"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[0.6039, 0.3689, 0.7573, 0.2495],\n",
            "        [0.6172, 0.8542, 0.2011, 0.8350],\n",
            "        [0.8214, 0.3384, 0.8358, 0.5788],\n",
            "        [0.5938, 0.9506, 0.2194, 0.4083]])\n",
            "First row:  tensor([0.6039, 0.3689, 0.7573, 0.2495])\n",
            "First column:  tensor([0.6039, 0.6172, 0.8214, 0.5938])\n",
            "Last column: tensor([0.2495, 0.8350, 0.5788, 0.4083])\n",
            "tensor([[0.6039, 0.0000, 0.7573, 0.2495],\n",
            "        [0.6172, 0.0000, 0.2011, 0.8350],\n",
            "        [0.8214, 0.0000, 0.8358, 0.5788],\n",
            "        [0.5938, 0.0000, 0.2194, 0.4083]])\n"
          ]
        }
      ],
      "source": [
        "tensor = torch.rand(4, 4)\n",
        "print(tensor)\n",
        "print('First row: ', tensor[0])\n",
        "print('First column: ', tensor[:, 0])\n",
        "print('Last column:', tensor[:, -1])\n",
        "\n",
        "## Assigning every element in second column to value 0\n",
        "\n",
        "tensor[:,1] = 0 \n",
        "print(tensor)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Cm9Aw1o2UrYX"
      },
      "source": [
        "### 4.8 Joining tensors: \n",
        "You can use ``torch.cat`` to concatenate a sequence of tensors along a given dimension.\n",
        "<!-- See also `torch.stack <https://pytorch.org/docs/stable/generated/torch.stack.html>`,\n",
        "another tensor joining op that is subtly different from ``torch.cat``. -->"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-wu19UpsUrYY",
        "outputId": "2975ca4e-a5a8-436d-e5a4-4acb0cd6c055"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([4, 4])\n",
            "torch.Size([4, 12])\n"
          ]
        }
      ],
      "source": [
        "tensor = torch.ones(4, 4)\n",
        "t1 = torch.ones(4, 4)\n",
        "print(t1.shape)\n",
        "t1 = torch.cat([tensor, tensor, tensor], dim=1)\n",
        "print(t1.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "40XU3eQVUrYZ"
      },
      "source": [
        "### 4.9 Arithmetic operations\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eXU_ZDUUUrYZ",
        "outputId": "5deb155b-0753-4fb9-cecf-b3994cb0917b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([2, 2])\n"
          ]
        }
      ],
      "source": [
        "# This computes the matrix multiplication between two tensors. \n",
        "shape = (2,3,)\n",
        "ones_tensor_1 = torch.ones(shape)\n",
        "ones_tensor_2 = torch.ones(shape)\n",
        "\n",
        "#y3 = torch.rand_like(tensor)\n",
        "y3=torch.matmul(ones_tensor_1, ones_tensor_2.T)\n",
        "\n",
        "print(y3.shape)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s2Lb3Vs3UrYa"
      },
      "source": [
        "###  4.10 Single-element tensors:\n",
        "If you have a one-element tensor, for example by aggregating all\n",
        "values of a tensor into one value, you can convert it to a Python\n",
        "numerical value using ``item()``:\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0tV-c9wdUrYa",
        "outputId": "d8730992-6bb4-4c6a-c57e-86bd2b9b5719"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "16.0 <class 'float'>\n"
          ]
        }
      ],
      "source": [
        "agg = tensor.sum()\n",
        "agg_item = agg.item()\n",
        "print(agg_item, type(agg_item))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1WU4YfInUrYb"
      },
      "source": [
        "### 4.11 PyTorch Broad casting\n",
        "\n",
        "General semantics: Two tensors are “broadcastable” if the following rules hold:\n",
        "\n",
        "*   Each tensor has at least one dimension.\n",
        "*   When iterating over the dimension sizes, starting at the trailing dimension, the dimension sizes must either be equal, one of them is 1, or one of them does not exist.\n",
        "\n",
        "If two tensors x, y are “broadcastable”, the resulting tensor size is calculated as follows:\n",
        "\n",
        "\n",
        "*   If the number of dimensions of x and y are not equal, prepend 1 to the dimensions of the tensor with fewer dimensions to make them equal length.\n",
        "*   Then, for each dimension size, the resulting dimension size is the max of the sizes of x and y along that dimension."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4y0CPtqeUrYc",
        "outputId": "8fb5947b-4f62-4efe-f9fc-a24e3dcb66ab"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([3, 1, 7])\n"
          ]
        }
      ],
      "source": [
        "# can line up trailing dimensions to make reading easier\n",
        "# x=torch.ones(5,1,4,1)\n",
        "# y=torch.ones(3,1,1)\n",
        "# print((x+y).size())\n",
        "# # #torch.Size([5, 3, 4, 1])\n",
        "\n",
        "# but not necessary:\n",
        "x=torch.ones(1)\n",
        "y=torch.ones(3,1,7)\n",
        "print((x+y).size())\n",
        "#torch.Size([3, 1, 7])\n",
        "\n",
        "# x=torch.ones(5,2,4,1)\n",
        "# y=torch.ones(3,1,1)\n",
        "# print((x+y).size())\n",
        "# # #RuntimeError: The size of tensor a (2) must match the size of tensor b (3) at non-singleton dimension 1"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6oQfzRlGUrYd"
      },
      "source": [
        "## 5 PyTorch Autograd"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y26MmQrEUrYh"
      },
      "source": [
        "In order to get familiar with the concept of a computation graph, we will create one for the following function:\n",
        "\n",
        "$$y = \\frac{1}{3}\\sum_{i=1}^3 \\left[(x_i + 2)^2 + 3\\right]$$\n",
        "\n",
        "Let us find the gradients $\\partial y / \\partial \\mathbf{x}$. For our example, we'll use $\\mathbf{x}=[0,1,2]$ as our input."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a9fe3S5JUrYi",
        "outputId": "2bf37925-8096-407f-a920-6671e190ca10"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "X tensor([0., 1., 2.], requires_grad=True)\n"
          ]
        }
      ],
      "source": [
        "x = torch.arange(3, dtype=torch.float32, requires_grad=True) # Only float tensors can have gradients\n",
        "print(\"X\", x)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# u = torch.randint(3, 5, (3,), dtype=torch.float32, requires_grad=True) # Only float tensors can have gradients\n",
        "# print(\"U\", u)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b2I0Fc9AjJbx",
        "outputId": "0537c314-6389-4f43-f547-9411afa9a066"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "U tensor([4., 4., 4.], requires_grad=True)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N2EUlfd9UrYj"
      },
      "source": [
        "Now let's build the computation graph step by step. You can combine multiple operations in a single line, but we will separate them here to get a better understanding of how each operation is added to the computation graph."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dVW9UMCjUrYk",
        "outputId": "3eb0d3ed-2061-4bd8-b6e5-56beaf4ae043"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Y tensor(12.6667, grad_fn=<MeanBackward0>)\n"
          ]
        }
      ],
      "source": [
        "a = x+2\n",
        "b = a ** 2\n",
        "c = b + 3\n",
        "y = c.mean()\n",
        "print(\"Y\", y)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "Using the statements above, we have created a computation graph that looks similar to the figure below:\n",
        "\n",
        "\n",
        "![image.png](data:image/png;base64,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)"
      ],
      "metadata": {
        "id": "3pGhDsqznm18"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We calculate $a$ based on the inputs $x$ and the constant $2$, $b$ is $a$ squared, and so on. The visualization is an abstraction of the dependencies between inputs and outputs of the operations we have applied.\n",
        "Each node of the computation graph has automatically defined a function for calculating the gradients with respect to its inputs, `grad_fn`. You can see this when we printed the output tensor $y$. This is why the computation graph is usually visualized in the reverse direction (arrows point from the result to the inputs). We can perform backpropagation on the computation graph by calling the function `backward()` on the last output, which effectively calculates the gradients for each tensor that has the property `requires_grad=True`:"
      ],
      "metadata": {
        "id": "m8QTuxKKnwDd"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "MGtGCGmDUrYl"
      },
      "outputs": [],
      "source": [
        "y.backward()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "x.grad"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZLAnoO0WlpWN",
        "outputId": "bfb9212c-9531-4982-b44b-86c1bc33505c"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([1.3333, 2.0000, 2.6667])"
            ]
          },
          "metadata": {},
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hfPMQ8poUrYl"
      },
      "source": [
        "`x.grad` will now contain the gradient $\\partial y/ \\partial \\mathcal{x}$, and this gradient indicates how a change in $\\mathbf{x}$ will affect output $y$ given the current input $\\mathbf{x}=[0,1,2]$:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8NBo4I-lUrYm",
        "outputId": "ff83218e-4348-444f-a74b-feee8a75d3bb"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([1.3333, 2.0000, 2.6667])\n"
          ]
        }
      ],
      "source": [
        "print(x.grad)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(u.grad)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "__oko354aSn6",
        "outputId": "046cc8ef-f984-4595-e18d-dfa87eea9bb3"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "None\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R-hYxQhGUrYm"
      },
      "source": [
        "We can also verify these gradients by hand. We will calculate the gradients using the chain rule, in the same way as PyTorch did it:\n",
        "\n",
        "$$\\frac{\\partial y}{\\partial x_i} = \\frac{\\partial y}{\\partial c_i}\\frac{\\partial c_i}{\\partial b_i}\\frac{\\partial b_i}{\\partial a_i}\\frac{\\partial a_i}{\\partial x_i}$$\n",
        "\n",
        "Note that we have simplified this equation to index notation, and by using the fact that all operation besides the mean do not combine the elements in the tensor. The partial derivatives are:\n",
        "\n",
        "$$\n",
        "\\frac{\\partial a_i}{\\partial x_i} = 1,\\hspace{1cm}\n",
        "\\frac{\\partial b_i}{\\partial a_i} = 2\\cdot a_i\\hspace{1cm}\n",
        "\\frac{\\partial c_i}{\\partial b_i} = 1\\hspace{1cm}\n",
        "\\frac{\\partial y}{\\partial c_i} = \\frac{1}{3}\n",
        "$$\n",
        "\n",
        "Hence, with the input being $\\mathbf{x}=[0,1,2]$, our gradients are $\\partial y/\\partial \\mathbf{x}=[4/3,2,8/3]$. The previous code cell should have printed the same result."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# external_grad = torch.tensor([1., 1., 1.])\n",
        "# b.backward(gradient=external_grad)\n",
        "# print(x.grad)"
      ],
      "metadata": {
        "id": "eMqXjpynljel"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## An example showing utility of Autograd."
      ],
      "metadata": {
        "id": "lxvEzHk1kHq6"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nYJYvKlgUrYd"
      },
      "source": [
        "We will use a problem of fitting $$y=\\sin(x)$$ with a third order polynomial as our running example. The network will have four parameters, and will be trained with gradient descent to fit random data by minimizing the Euclidean distance between the network output and the true output. We need to manually implement the forward and backward passes through the network.\n",
        "$$y=a + b x + c x^ 2 + d x ^3$$ \n",
        "\n",
        "We have a set of data points consisting of independent variables $x_i$ and corresponding observed output $y_i$. The set is represented as $\\{(x_0,y_0),(x_1,y_1),(x_2,y_2),\\dots(x_N,y_N)\\}$. Also, corresponding to each $x_i$ we have a predicted value $y_i^{pred}$.\n",
        "The error is represented as \n",
        "$$\\mathcal{L}=\\sum_{i=1}^N {(y_i-y^{pred}_i)}^2=\\sum_{i=1}^N {(y_i-a - b x_i - c x_i^ 2 - d x_i^3)}^2  $$\n",
        "\n",
        "$$\\frac{\\partial\\mathcal{L}}{\\partial a}=\\frac{\\partial}{\\partial a}\\sum_{i=1}^N {(y_i-y^{pred}_i)}^2=-2*\\sum_{i=1}^N {(y_i-y^{pred}_i)}$$\n",
        "\n",
        "$$a^{new}=a^{old}-\\eta\\frac{\\partial\\mathcal{L}}{\\partial a}$$\n",
        "\n",
        "$$\\frac{\\partial\\mathcal{L}}{\\partial b}=\\frac{\\partial}{\\partial b}\\sum_{i=1}^N {(y_i-y^{pred}_i)}^2=-2*\\sum_{i=1}^N {x_i*(y_i-y^{pred}_i)}$$\n",
        "\n",
        "$$b^{new}=b^{old}-\\eta\\frac{\\partial\\mathcal{L}}{\\partial b}$$\n",
        "\n",
        "\n",
        "<!-- Actual taylors expansion of $sin(x)$\n",
        "$$sin(x)=x - \\frac{x^3}{3!}  + \\frac{x^5}{5!} - \\frac{x^7}{7!}+\\dots$$  -->"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZLWy92TQUrYe",
        "outputId": "5dc9a6a8-4244-478b-e95e-b564a68b47ab"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch= 99 loss= 441.12701416015625\n",
            "epoch= 199 loss= 310.9689636230469\n",
            "epoch= 299 loss= 220.1435546875\n",
            "epoch= 399 loss= 156.71507263183594\n",
            "epoch= 499 loss= 112.38798522949219\n",
            "epoch= 599 loss= 81.38880920410156\n",
            "epoch= 699 loss= 59.69626235961914\n",
            "epoch= 799 loss= 44.50689697265625\n",
            "epoch= 899 loss= 33.86479187011719\n",
            "epoch= 999 loss= 26.404468536376953\n",
            "epoch= 1099 loss= 21.171798706054688\n",
            "epoch= 1199 loss= 17.499732971191406\n",
            "epoch= 1299 loss= 14.921594619750977\n",
            "epoch= 1399 loss= 13.110649108886719\n",
            "epoch= 1499 loss= 11.838059425354004\n",
            "epoch= 1599 loss= 10.943389892578125\n",
            "epoch= 1699 loss= 10.314180374145508\n",
            "epoch= 1799 loss= 9.871498107910156\n",
            "epoch= 1899 loss= 9.559940338134766\n",
            "epoch= 1999 loss= 9.340593338012695\n",
            "Result: y = -0.023479457944631577 + 0.8512952923774719 x + 0.004050595220178366 x^2 + -0.09255580604076385 x^3\n"
          ]
        }
      ],
      "source": [
        "import math\n",
        "\n",
        "\n",
        "dtype = torch.float\n",
        "# device = torch.device(\"cpu\")\n",
        "device = torch.device(\"cuda:0\") # Uncomment this to run on GPU\n",
        "\n",
        "# Create random input and output data\n",
        "x = torch.linspace(-math.pi, math.pi, 2000, device=device, dtype=dtype)\n",
        "y = torch.sin(x)\n",
        "\n",
        "# Randomly initialize weights\n",
        "a = torch.randn((), device=device, dtype=dtype)\n",
        "b = torch.randn((), device=device, dtype=dtype)\n",
        "c = torch.randn((), device=device, dtype=dtype)\n",
        "d = torch.randn((), device=device, dtype=dtype)\n",
        "#print(\"a\",a)\n",
        "\n",
        "learning_rate = 1e-6\n",
        "for t in range(2000):\n",
        "    # Forward pass: compute predicted y\n",
        "    y_pred = a + b * x + c * x ** 2 + d * x ** 3\n",
        "\n",
        "    # Compute and print loss\n",
        "    loss = (y -y_pred ).pow(2).sum().item()\n",
        "    if t % 100 == 99:\n",
        "        print(\"epoch=\",t, \"loss=\",loss)\n",
        "\n",
        "    # Backprop to compute gradients of a, b, c, d with respect to loss\n",
        "    grad_y_pred = 2.0 * (y -y_pred)\n",
        "    grad_a = (-1)*grad_y_pred.sum()\n",
        "    grad_b = (-1)*(grad_y_pred * x).sum()\n",
        "    grad_c = (-1)*(grad_y_pred * x ** 2).sum()\n",
        "    grad_d = (-1)*(grad_y_pred * x ** 3).sum()\n",
        "\n",
        "    # Update weights using gradient descent\n",
        "    a -= learning_rate * grad_a\n",
        "    b -= learning_rate * grad_b\n",
        "    c -= learning_rate * grad_c\n",
        "    d -= learning_rate * grad_d\n",
        "\n",
        "print(f'Result: y = {a.item()} + {b.item()} x + {c.item()} x^2 + {d.item()} x^3')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fxjvONz4UrYf"
      },
      "source": [
        "\n",
        "In the above examples, we had to manually implement both the forward and backward passes of our model. Manually implementing the backward pass is not a big deal for a small model, but can quickly get very complicated for large complex models.\n",
        "\n",
        "\n",
        "Here we use PyTorch Tensors and `autograd` to implement our fitting sine wave with third order polynomial example; now we no longer need to manually implement the backward pass through the network:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "id": "8D9E-gYeUrYf",
        "outputId": "882c1b92-f5cd-48b0-96df-f592a53ef4ee",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "99 1166.781494140625\n",
            "199 801.1495971679688\n",
            "299 551.5908203125\n",
            "399 381.0613708496094\n",
            "499 264.4001159667969\n",
            "599 184.4993133544922\n",
            "699 129.71249389648438\n",
            "799 92.1033706665039\n",
            "899 66.25664520263672\n",
            "999 48.473663330078125\n",
            "1099 36.22513961791992\n",
            "1199 27.779281616210938\n",
            "1299 21.9493465423584\n",
            "1399 17.920801162719727\n",
            "1499 15.134149551391602\n",
            "1599 13.204569816589355\n",
            "1699 11.86716365814209\n",
            "1799 10.939294815063477\n",
            "1899 10.294957160949707\n",
            "1999 9.847097396850586\n",
            "Result: y = 0.03001800738275051 + 0.8421373963356018 x + -0.005178605206310749 x^2 + -0.09125317633152008 x^3\n"
          ]
        }
      ],
      "source": [
        "# -*- coding: utf-8 -*-\n",
        "import torch\n",
        "import math\n",
        "\n",
        "dtype = torch.float\n",
        "device = torch.device(\"cpu\")\n",
        "# device = torch.device(\"cuda:0\")  # Uncomment this to run on GPU\n",
        "\n",
        "# Create Tensors to hold input and outputs.\n",
        "# By default, requires_grad=False, which indicates that we do not need to\n",
        "# compute gradients with respect to these Tensors during the backward pass.\n",
        "x = torch.linspace(-math.pi, math.pi, 2000, device=device, dtype=dtype)\n",
        "y = torch.sin(x)\n",
        "\n",
        "# Create random Tensors for weights. For a third order polynomial, we need\n",
        "# 4 weights: y = a + b x + c x^2 + d x^3\n",
        "# Setting requires_grad=True indicates that we want to compute gradients with\n",
        "# respect to these Tensors during the backward pass.\n",
        "a = torch.randn((), device=device, dtype=dtype, requires_grad=True)\n",
        "b = torch.randn((), device=device, dtype=dtype, requires_grad=True)\n",
        "c = torch.randn((), device=device, dtype=dtype, requires_grad=True)\n",
        "d = torch.randn((), device=device, dtype=dtype, requires_grad=True)\n",
        "\n",
        "\n",
        "learning_rate = 1e-6\n",
        "for t in range(2000):\n",
        "    # Forward pass: compute predicted y using operations on Tensors.\n",
        "    y_pred = a + b * x + c * x ** 2 + d * x ** 3\n",
        "\n",
        "    # Compute and print loss using operations on Tensors.\n",
        "    # Now loss is a Tensor of shape (1,)\n",
        "    # loss.item() gets the scalar value held in the loss.\n",
        "    loss = (y - y_pred).pow(2).sum()\n",
        "    if t % 100 == 99:\n",
        "        print(t, loss.item())\n",
        "\n",
        "    # Use autograd to compute the backward pass. This call will compute the\n",
        "    # gradient of loss with respect to all Tensors with requires_grad=True.\n",
        "    # After this call a.grad, b.grad. c.grad and d.grad will be Tensors holding\n",
        "    # the gradient of the loss with respect to a, b, c, d respectively.\n",
        "    loss.backward()\n",
        "\n",
        "    # Manually update weights using gradient descent. Wrap in torch.no_grad()\n",
        "    # because weights have requires_grad=True, but we don't need to track this\n",
        "    # in autograd.\n",
        "    with torch.no_grad():\n",
        "        a -= learning_rate * a.grad\n",
        "        b -= learning_rate * b.grad\n",
        "        c -= learning_rate * c.grad\n",
        "        d -= learning_rate * d.grad\n",
        "\n",
        "        # Manually zero the gradients after updating weights\n",
        "        a.grad = None\n",
        "        b.grad = None\n",
        "        c.grad = None\n",
        "        d.grad = None\n",
        "\n",
        "print(f'Result: y = {a.item()} + {b.item()} x + {c.item()} x^2 + {d.item()} x^3')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CkCOhKMxUrYn"
      },
      "source": [
        "## References\n",
        "<ul>\n",
        "  <li><a href=\"https://github.com/haofeixu/cs231n/blob/master/assignment2/PyTorch.ipynb\">cs231n</a></li>\n",
        "  <li><a href=\"https://pytorch.org/tutorials\">PyTorch Tutorials</a></li>\n",
        "  <li><a href=\"https://github.com/jcjohnson/cnn-benchmarks\">CPU GPU speedup comparison</a></li>\n",
        "  <li><a href=\"https://uvadlc-notebooks.readthedocs.io/en/latest/tutorial_notebooks/tutorial2/Introduction_to_PyTorch.html\">Computation graph</a></li>\n",
        "</ul>"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.5"
    },
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}